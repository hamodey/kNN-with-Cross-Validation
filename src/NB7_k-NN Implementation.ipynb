{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 7. k-NN Implementation\n",
    "\n",
    "**Instructions:**\n",
    "* go through the notebook and complete the **tasks** .  \n",
    "* Make sure you understand the examples given!\n",
    "* When a question allows a free-form answer (e.g., ``what do you observe?``) create a new markdown cell below and answer the question in the notebook.\n",
    "* ** Save your notebooks when you are done! **\n",
    "\n",
    "In this lab, you will try to implement your own k-NN classifier using numpy functions.  \n",
    "\n",
    "\n",
    "**Note** You can always copy the code in a separate notebook (or, a plain text file .py that you can run with python from the command line) if you want.  After you are done, you can copy the code back in this notepad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Run the cell below to load our data. This piece of code is exactly the same as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import k-nn classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import operator\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#view a description of the dataset (uncomment next line to do so)\n",
    "#print(iris.DESCR)\n",
    "\n",
    "#Set X equal to features, Y equal to the targets\n",
    "\n",
    "X=iris.data \n",
    "y=iris.target \n",
    "\n",
    "\n",
    "mySeed=1234567\n",
    "#initialize random seed generator \n",
    "np.random.seed(mySeed)\n",
    "\n",
    "#we add some random noise to our data to make the task more challenging\n",
    "X=X+np.random.normal(0,0.5,X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> The code below splits our data into two sets (a training and testing set), and subsequently trains a scikit-learn classifier on the training data and tests on the testing data.  To avoid complicating things, in this lab you just need to follow this setting, no need to consider cross-validation.  We are also using a fixed number of neighbours (10) and the euclidean distance.  You can just run the cell below and make sure you understand the code - nothing else to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(mySeed)\n",
    "indices= np.random.permutation(X.shape[0]) \n",
    "bins=np.array_split(indices,2) # we  just need a training and testing set here\n",
    "foldTrain=bins[0]\n",
    "foldTest=bins[1]\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "knn.fit(X[foldTrain],y[foldTrain])\n",
    "y_pred=knn.predict(X[foldTest])\n",
    "print(accuracy_score(y[foldTest],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Your task is now to implement your own version of k-NN, based on the lecture slides and the description given here.  A suggested structure for doing this is included in the comments below, but feel free to start working in a different cell or in your favourite IDE.  We are still using a simple training/test split (no cross-validation here) to avoid complicating things, and thus use a fixed number of neighbours (10) and the euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## ANSWER HERE: Suggested code structure in comments below\n",
    "# given a test point, your code should\n",
    "# \n",
    "# - get the 'nearest neighbours' - i.e. the samples in the training set - that are nearest to our test sample\n",
    "# -----> done by evaluating the distance of the test sample to all samples in the training set\n",
    "# - assign a label to the test sample based on the 'neighbours'\n",
    "\n",
    "##=== FUNCTION DEFINITIONS  ===##\n",
    "\n",
    "\n",
    "#define distance functions: given two vectors (ndarrays), this function returns the distance between them\n",
    "#Write at least two distance functions, measuring the squared distance between your data and the absolute value distance.\n",
    "#You can implement both of these by looking at the numpy.linalg.norm method, or implement your own version.  \n",
    "def euclideanDistance(in1,in2):\n",
    "    return ##eucledian distance between in1 and in2##\n",
    "\n",
    "\n",
    "#The get neighbours function  returns the nearest neighbour indices in X of the test point x_.  In more detail\n",
    "# Input: x_ : point in test data\n",
    "#       X   : training data\n",
    "#       n   : number of neighbours to return\n",
    "#       T   : total number of training data\n",
    "# Output: n-nearest neighbours of x_ in training data X\n",
    "\n",
    "def getNeighbours(x_,X,n,T): # where T is number of data\n",
    "    return # indices of n-nearest neighbours in training data\n",
    "\n",
    "\n",
    "# The assign label function returns the assigned label for a test data point, given the labels of nearest neighbours\n",
    "# Input: nLabels : labels (classes) of nearest neighbours of a test point\n",
    "# Output: the assigned label\n",
    "# e.g., if we have n=1 (one neighbour), then we can just return the label of the nearest neighbour\n",
    "# else, we can e.g., choose the majority class\n",
    "def assignLabel(nLabels):\n",
    "    return # label assigned to test point x_\n",
    "\n",
    "##=== FUNCTION DEFINITIONS (END)  ===##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# here is some sample code for evaluating the kNN classifier you just built\n",
    "# NOTE: this is just a suggested way to do this - you can do it in another way if you want\n",
    "correct=0;\n",
    "for i in foldTest: #for all test points\n",
    "    # knn classifier\n",
    "    x_=X[i] # test point x_\n",
    "    y_=y[i] # true label for y_\n",
    "    \n",
    "    # get neighbours of x_ in training data \n",
    "    # assignLabel to x_ based on neighbours\n",
    "    # evaluate if the assigned label is correct (equal to y_)\n",
    "    \n",
    "#print accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
